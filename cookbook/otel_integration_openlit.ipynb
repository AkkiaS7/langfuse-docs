{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "description: Example cookbook for the OpenLIT Langfuse integration using OpenTelemetry.\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# OpenLIT Integration via OpenTelemetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8xR_verv0yp8",
        "outputId": "1bf27d03-bc6a-4fda-e486-a049534cdc09"
      },
      "outputs": [],
      "source": [
        "%pip install openai langfuse openlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1-vH4lFi6t-D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "LANGFUSE_PUBLIC_KEY=\"\"\n",
        "LANGFUSE_SECRET_KEY=\"\"\n",
        "LANGFUSE_AUTH=base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # EU data region\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # US data region\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMaAjHXSi0Kr",
        "outputId": "277ddaa5-8de1-47f8-b5d1-d4df5dbd448a"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import openlit\n",
        "\n",
        "openlit.init(disable_batch=True)\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"What is LLM Observability?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWrLi7RltrrK"
      },
      "source": [
        "[Example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/64902f6a5b4f27738be939b7ad38eab3?timestamp=2025-02-02T22%3A09%3A53.053Z)\n",
        "\n",
        "![OpenLIT OpenAI Trace](https://langfuse.com/images/cookbook/otel-integration-openlit/openlit-openai-trace.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
